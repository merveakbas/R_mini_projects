---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(lubridate)
# Read in the data
#click_data <- read_csv("click_data.csv")

# Calculate the mean conversion rate by day of the week
conv_byday <- click_data %>% mutate(dow = weekdays(visit_date)) %>%
  group_by(dow) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))
  
 ggplot(conv_byday, aes(x=dow, y = conversion_rate) ) +
 geom_point() + geom_line()
```
Earlier we said that based on the summary by month, it looks like there are seasonal effects. Conversion rates are higher in the summer and in December. Let's visualize that effect, but by week of the year. We'll use ggplot2 to build our plot. 
```{r}
# Compute conversion rate by week of the year
click_data_sum <- click_data %>%
  group_by(week(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Build plot
ggplot(click_data_sum, aes(x = `week(visit_date)`,
                           y = conversion_rate)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1),
                     labels = percent)
```


#POWER ANALYSIS
#Determine the experiment length:
Power analysis tells you how many data points you need to collect to know your effect is real. 
Once you know the data points, you will know how many days you need to collect data.
Know: 
Planed statistical test
The value control condition
The desired alue of the test condition
alpha:0.05
power:0.8


```{r}
library(powerMediation)
help(SSizeLogisticBin)
```

SSizeLogisticBin(p1, 
                 p2, 
                 B, 
                 alpha = 0.05, 
                 power = 0.8)
                 
Where p1 is the baseline rate of the control condition,
p2 is the expected outcome for the test condition, 
B is the proportion of test group to total sampe size,

```{r}
# Load powerMediation
head(click_data_month)
names(click_data_month) <- c("month", "conversion_rate")
library(powerMediation)

# Compute and look at sample size for experiment in August
p_aug <- round(as.numeric(click_data_month %>% filter(month =="Aug") %>% select(conversion_rate)),2)
p_aug

total_sample_size <- SSizeLogisticBin(p1 = p_aug,
                                      p2 = 0.6,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```


For each observation we hav ethe click or nonclick info and the condition
The summary dataset below creates a conversion rate for each day for each condition. 
If data was corrupted one day we drop that data

```{r}
# Group and summarize data
experiment_data_clean_sum <- experiment_data_clean %>%
  group_by(condition, visit_date) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Make plot of conversion rates over time
ggplot(experiment_data_clean_sum,
       aes(x = visit_date,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point() +
  geom_line()
```

Then we run the logistic regression with a binary predictor (which is the condition dummy)
```{r}
# Load package for cleaning model results
library(broom)

# View summary of results
experiment_data_clean %>%
  group_by(condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
experiment_results <- glm(clicked_adopt_today ~ condition,
                          family = "binomial",
                          data = experiment_data_clean) %>%
  tidy()
experiment_results
```

Confounding variables: 
The previous experiment tested if showing a picture of an adult cat with a hat increased the adoption rate. 

Let's start your kitten experiment. The hat already increased conversion rates a lot, but you think making the photo a kitten will really make the difference, so you think conversion rates will go up to 59%. Let's run a power analysis to see how much data you need to see a significant effect.
```{r}
# Load package for running power analysis
library(powerMediation)

# Run logistic regression power analysis
total_sample_size <- SSizeLogisticBin(p1 = 0.39,
                                      p2 = 0.59,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

RESULT is 194

We don't need a lot of data points because the difference we expect in the outcome variable is 
so huge!

Logistic regression
```{r}
# Read in data for follow-up experiment
followup_experiment_data <- read_csv("followup_experiment_data.csv")
head(followup_experiment_data)

# View conversion rates by condition
followup_experiment_data %>%
  group_by(condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))


# Run logistic regression
followup_experiment_results <- glm(clicked_adopt_today ~ condition,
                                   family = "binomial",
                                   data =   followup_experiment_data) %>%
  tidy()
followup_experiment_results
```

Results:
 condition  conversion_rate
  <chr>                <dbl>
1 cat_hat              0.814
2 kitten_hat           0.876

  term  estimate std.error statistic      p.value
1         (Intercept) 1.4790761 0.2611777  5.663103 1.486597e-08
2 conditionkitten_hat 0.4786685 0.4041175  1.184479 2.362236e-01

You correctly found that the follow-up experiment didn't work (our p-value was about 0.24, which is not less than 0.05). This could be because kittens aren't actually that desirable, or because we went in with bad assumptions. We found our conversion results in our first experiment in January, but ran our second experiment in August, when conversion rates were already high. Remember to always consider what 'control' really means when building your follow-up experiments.

```{r}
# Compute monthly summary
head(eight_month_checkin_data)

eight_month_checkin_data_sum <- eight_month_checkin_data %>%
  mutate(month_text = month(visit_date, label = TRUE)) %>%
  group_by(month_text, condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Plot month-over-month results
ggplot(eight_month_checkin_data_sum,
       aes(x = month_text,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point() +
  geom_line()
```



```{r}
# Plot monthly summary
ggplot(eight_month_checkin_data_sum,
       aes(x = month_text,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1),
                     labels = percent) +
  labs(x = "Month",
       y = "COnversion Rate")
```

NICER PLOT

```{r}
# Plot monthly summary
ggplot(eight_month_checkin_data_sum,
       aes(x = month_text,
           y = conversion_rate,
           color = condition,
           group = condition)) +
  geom_point(size = 4) + #makes points bigger
  geom_line(lwd = 1) +  #makes lines thicker
  scale_y_continuous(limits = c(0, 1),
                     labels = percent) +
  labs(x = "Month",
       y = "Conversion Rate")
```

We should also check if conversion rates have changed between years for our no-hat condition.
```{r}
# Compute difference over time
no_hat_data_diff <- no_hat_data_sum %>%
  spread(year, conversion_rate) %>%
  mutate(year_diff = `2018` - `2017`)
no_hat_data_diff

# Compute summary statistics
mean(no_hat_data_diff$year_diff, na.rm = T)
sd(no_hat_data_diff$year_diff, na.rm = T)
```


Re-run power analysis for follow-up
Let's rerun our power analysis for our new experiment now taking into consideration the time of year we're running on new experiment: September. To figure out our baseline assumptions, we'll give you some introdutory information: 1) the conversion rate for the no hat condition in 2017 was 30% (or 0.3), and 2) the averge difference bewteen the no hat condition and the cat hat condition is 19% (0.19). Use this information to run an updated power analysis.

Use 0.49 as baseline conversion rate - and expect a 0.15 increase

```{r}
# Load package for power analysis
library(powerMediation)

# Run power analysis for logistic regression
total_sample_size <- SSizeLogisticBin(p1 = 0.49,
                                      p2 = 0.64,
                                      B = 0.5,
                                      alpha = 0.05,
                                      power = 0.8)
total_sample_size
```

FOllow up Experiment results
```{r}
# Load package to clean up model outputs
library(broom)

# View summary of data
followup_experiment_data_sep %>%
  group_by(condition) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
followup_experiment_sep_results <- glm(clicked_adopt_today ~ condition,
                                       family = "binomial",
                                       data = followup_experiment_data_sep) %>%
  tidy()
followup_experiment_sep_results
```

 condition  conversion_rate
  <chr>                <dbl>
1 cat_hat              0.468
2 kitten_hat           0.614

   term   estimate std.error  statistic     p.value
1         (Intercept) -0.1288329 0.1532613 -0.8406096 0.400566704
2 conditionkitten_hat  0.5931385 0.2194637  2.7026718 0.006878462

#NOW EXPLORING CONVERSION RATES

```{r}
# Compute summary by month
viz_website_2017 %>%
  group_by(month(visit_date)) %>%
  summarize(article_conversion_rate = mean(clicked_article))
```

```{r}
# Compute 'like' click summary by month
viz_website_2017_like_sum <- viz_website_2017 %>%
  mutate(month = month(visit_date, label = TRUE)) %>%
  group_by(month) %>%
  summarize(like_conversion_rate = mean(clicked_like))

# Plot 'like' click summary by month
ggplot(viz_website_2017_like_sum,
       aes(x = month, y = like_conversion_rate, group = 1)) + #group tells which points to connect with the line
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1), labels = percent)
```


```{r}
# Plot comparison of 'like'ing and 'sharing'ing an article
ggplot(viz_website_2017_like_share_sum,
       aes(x = month, y = conversion_rate, color = action, group = action)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1), labels = percent)
```


withnroup variation - has more power
between group variation

Example of plotting A/A experiment
```{r}
# Compute conversion rates for A/A experiment
viz_website_2018_01_sum <- viz_website_2018_01 %>%
  group_by(condition) %>%
  summarize(like_conversion_rate = mean(clicked_like))

viz_website_2018_01_sum

# Plot conversion rates for two conditions
ggplot(viz_website_2018_01_sum,
       aes(x = condition, y = like_conversion_rate)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, 1), labels = percent)
```

Logistic regression to compare aa tets results - it should be nonsignificant
```{r}
# Load library to clean up model outputs
library(tidyverse)
library(broom)

# Run logistic regression
aa_experiment_results <- glm(clicked_like ~ condition,
                             family = "binomial",
                             data = viz_website_2018_01) %>%
  tidy()
aa_experiment_results
```

 aa_experiment_results
         term    estimate  std.error   statistic   p.value
1 (Intercept) -1.38025691 0.02004420 -68.8606672 0.0000000
2 conditionA2 -0.02591398 0.02845802  -0.9106038 0.3625041

CONFOUNDING VARIABLES
Some confounding variables are internal. 
For example let's assume we are testing whether the title tips for blah vs tools for blah. 
Let's say that you found that tips performed better. Is it just because people like tips or because tips is shorter - or it is a less common word. This is an internal confounding variable. 

Let's say that you found this effect one month and the next month you find a different result.
Maybe the age group between the two months have changed. and the first month's age was younger than the second month. The demographic variable confounded your experiment because the effect you found earlier was valid for only to a particular age group of your whole reader base. And did not have external validity.  


New example:

You have decided to run your "Tips" versus "Tools" experiment and look at percentage of 'like's. You run the experiment over the month of February. In the later half of February, an article goes viral called "Tips Just Get you Started, but Tools Open all the Doors". Let's see how this article affected results.

To compute the conversion rate depending on if the article was published or not, group by the column that codes if the article is published or not.

```{r}
head(viz_website_2018_02)

# Compute 'like' conversion rate by week and condition
viz_website_2018_02 %>%
  mutate(week = week(visit_date)) %>%
  group_by(week, condition) %>%
  summarize(like_conversion_rate = mean(clicked_like))

# Compute 'like' conversion rate by if article published and condition
viz_website_2018_02 %>%
  group_by(article_published, condition) %>%
  summarize(like_conversion_rate = mean(clicked_like))
```


  article_published condition like_conversion_rate
  <chr>             <chr>                    <dbl>
1 no                tips                    0.118 
2 no                tools                   0.0200
3 yes               tips                    0.119 
4 yes               tools                   0.106

so after the article is published - tools jumps up

Confounding variable example plotting
Let's see if we can tell when 'like' rates really started to change by plotting daily like rates.

ADVANCED PLOTTING 
```{r}
viz_website_2018_02_sum %>% filter(article_published == "yes") %>% ungroup() %>% summarize(firstday = min(visit_date))


# Plot 'like' conversion rates by date for experiment
ggplot(viz_website_2018_02_sum,
       aes(x = visit_date,
           y = like_conversion_rate,
           color = condition,
           linetype = article_published,
           group = interaction(condition, article_published))) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = as.numeric(as.Date("2018-02-15"))) +
  scale_y_continuous(limits = c(0, 0.3), labels = percent)
```


#SIDE EFFECTS
A side effect is an unintended consequence of a change you made. 
For example you change the word from tools to tips and tips loads in 3 seconds less. 
So any change that slows down the site would have a side effect. 
COmmon side effects are:
Site load time 
And amount of information "above the fold"

Side effect load time plot
The viral article has died down, so you've decided to rerun your experiment on "tips" vs. "tools". However, after running it for a month you realized there was a larger load delay for the "tools" homepage than the "tips" homepage. You then added a delay to the "tips" homepage so that they even out. To start, let's visualize the effect the delay has on a 'like' rates on a daily basis.


The viral article has died down, so you've decided to rerun your experiment on "tips" vs. "tools". However, after running it for a month you realized there was a larger load delay for the "tools" homepage than the "tips" homepage. You then added a delay to the "tips" homepage so that they even out. To start, let's visualize the effect the delay has on a 'like' rates on a daily basis.
```{r}
# Compute 'like' conversion rate and mean pageload time by day
viz_website_2018_03_sum <- viz_website_2018_03 %>%
  group_by(visit_date, condition) %>%
  summarize(mean_pageload_time = mean(pageload_time),
            like_conversion_rate = mean(clicked_like))

# Plot effect of 'like' conversion rate by pageload time
ggplot(viz_website_2018_03_sum,
       aes(x = mean_pageload_time, y = like_conversion_rate, color = condition)) +
  geom_point()
```

```{r}
# Plot 'like' conversion rate by day
ggplot(viz_website_2018_03_sum,
       aes(x = visit_date,
           y = like_conversion_rate,
           color = condition,
           linetype = pageload_delay_added,
           group = interaction(condition, pageload_delay_added))) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = as.numeric(as.Date("2018-03-15"))) +
  scale_y_continuous(limits = c(0, 0.3), labels = percent)
```


#CHAPTER 4: POWER ANALYSES

Power: Rejecting null hypothesis when it is false
Alpha; Rejecting null when it is true
Beta: Accepting null when it is false 

Effect size: Differnce in means between the groups divided by the pooled estimate of the common standard deviation


Doing a power analysis for t-test 
d is the effect size
```{r}
library(pwr)


pwr.t.test(power = 0.8,
           sig.level = 0.05,
           d = 0.6)
```

   Two-sample t test power calculation 

              n = 44.58577
              d = 0.6
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

NOTE: n is number in *each* group


##I SKIPPED
T-TEST AND LOGISTIC REGRESSION

#TEST POWER ANALYSIS
```{r}
# Load package to run power analysis
library(pwr)

# Run power analysis for t-test
sample_size <- pwr.t.test(d = 0.3,  #d is the expected effect size
                          sig.level = 0.05,
                          power = 0.8)
sample_size
```

```{r}
# Run t-test
ab_experiment_results <- t.test(time_spent_homepage_sec ~ condition,
                                data = viz_website_2018_04)
ab_experiment_results
```


#SEQUENTIAL ANALYSIS
What is a stopping rule? - Cambridge Dictionary of Statistics
Sequential analysis: A procedure in which a statistical test of significance is conducted repeatedly over time as the data are collected. After each observation, the cumulative data are analyzed and one of the following three decisions taken:

stop the data collection, reject the null hypothesis and claim statistical significance;
stop the data collection, do not reject the null hypothesis and state that the results are not statistically significant;
continue the data collection, since as yet the cumulated data are inadequate to draw a conclusion.


In this analysis you come up with a new p value to look at results multiple times
We use Use Pocock as our method for the spending function
```{r}
# Load package to run sequential analysis

library(gsDesign)
# Run sequential analysis
seq_analysis_3looks <- gsDesign(k = 3, #number of times you look at data
                               test.type = 1,
                               alpha = 0.05,
                               beta = 0.2,
                               sfu = "Pocock")
seq_analysis_3looks
```

Do a sequential analysis
Set the maximum data points 
Come up with Stopping points

```{r}
# Load package to run sequential analysis
library(gsDesign)

# Run sequential analysis
seq_analysis_3looks <- gsDesign(k = 3,
                                test.type = 1,
                                alpha = 0.05,
                                beta = 0.2,
                                sfu = "Pocock")

# Fill in max number of points and compute points per group and find stopping points
max_n <- 3000
max_n_per_group <- max_n / 2
stopping_points <- max_n_per_group * seq_analysis_3looks$timing
stopping_points
```




#MULTIVARIATE TESTING

Let's imagine we are testing 
Tips for better something  x Tools for Amazing something 
You have two levels:
Word one: tips vs tools
Word two: Better vs Amazing

library(broom)

multivar_results <- lm(time_spent_homepage_sec ~ word_one * word_two,
                       data = viz_website_2018_05) %>%
  tidy()
multivar_results
################################################

term    estimate   std.error  statistic   p.value
1                  (Intercept) 48.00829170 0.008056696 5958.80671 0.0000000
2                word_onetools  4.98549854 0.011393888  437.55902 0.0000000
3               word_twobetter -0.01323206 0.011393888   -1.16133 0.2455122
4 word_onetools:word_twobetter -4.97918356 0.016113391 -309.00904 0.0000000

#
The resulst we see in this table are in comparison to baseline values. For example, the difference between tools and tips is significant when the second word is "amazing", the baseline valeu. 

The difference between better and amazing is not significant when the first word is tips. 

Given that the interaction term is significant, thsi means that the efefct of one  test group changes with respect to the level of the other independent variable. 

    term     estimate   std.error    statistic   p.value
1                   (Intercept) 47.995059637 0.008056696 5957.1643430 0.0000000
2                 word_onetools  0.006314972 0.011393888    0.5542421 0.5794152
3               word_twoamazing  0.013232063 0.011393888    1.1613299 0.2455122
4 word_onetools:word_twoamazing  4.979183565 0.016113391  309.0090419 0.0000000

#Plotting time homepage in multivariate experiment
In the video, I ran our statistical analysis but didn't plot our data. Remember, it's always important to plot your data first so you're sure you have a sense of what's going on. Let's plot the means for our four conditions for time spent on the homepage.


PLOT HOMEPAGE VISIT LENGTH
```{r}
# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>%
  group_by(word_one, word_two) %>%
  summarize(mean_time_spent_homepage_sec = mean(time_spent_homepage_sec))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum,
       aes(x = word_one,
           y = mean_time_spent_homepage_sec,
           fill = word_two)) +
  geom_bar(stat = "identity", position = "dodge")
```


PLOT MEAN CLICK RATE 
```{r}
# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>%
  group_by(word_one, word_two) %>%
  summarize(like_conversion_rate = mean(clicked_like))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum,
       aes(x = word_one,
           y = like_conversion_rate,
           fill = word_two)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_continuous(limits = c(0, 1), labels = percent)
```

```{r}
#STATS ANALYSIS FOR CLICK RATE

# Load package for cleaning model output
library(broom)

# Organize variables and run logistic regression
viz_website_2018_05_like_results <- viz_website_2018_05 %>%
  mutate(word_one = factor(word_one,
                           levels = c("tips", "tools"))) %>%
  mutate(word_two = factor(word_two,
                           levels = c("better", "amazing"))) %>%
  glm(clicked_like ~ word_one * word_two,
                                    family = "binomial",
                                    data = .) %>%
  tidy()
viz_website_2018_05_like_results
```




